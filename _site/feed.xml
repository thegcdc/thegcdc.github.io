<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Digital Commoners</title>
    <description>building infrastructures for the common governance of digital resources</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 18 Sep 2020 11:37:28 +0200</pubDate>
    <lastBuildDate>Fri, 18 Sep 2020 11:37:28 +0200</lastBuildDate>
    <generator>Jekyll v4.1.1</generator>
    
      <item>
        <title>Data Trusts: Why, What and How</title>
        <description>&lt;p&gt;How do we, the general public, gain greater control over the estimated 2.5 quintillion bytes of data that is recorded, stored, processed and analysed, every day? For the moment, we have little say over what can be collected, accessed and used, and by whom. Nor do we enjoy much agency over the ways social platforms study and steer our behaviors. Let’s take Uber, if Uber does something you — a regular user — do not like, this isn’t something Uber views as up for discussion. Your only recourse is to delete the app. Your act of defiance is unlikely to have a large impact. If you can even afford to that is; what if Uber was your only way to get to work?&lt;/p&gt;

&lt;p&gt;In this article I put forward the concept of &lt;strong&gt;data trusts&lt;/strong&gt; as a way to claw back some control over the digital utilities that we rely on for our everyday lives. A data trust is a structure whereby data is placed under the control of a board of trustees with a fiduciary responsibility to look after the interests of the beneficiaries — you, me, society. Using them offers all of us the chance of a greater say in how our data is collected, accessed and used by others. This goes further than limiting data collecting and access to protect our privacy; it promotes the beneficial use of data, and ensures these benefits are widely felt across society&lt;/p&gt;

&lt;p&gt;In a sense, data trusts are to the data economy what trade unions are to the labour economy.&lt;/p&gt;

&lt;h2 id=&quot;whos-in-control&quot;&gt;Who’s in control?&lt;/h2&gt;

&lt;p&gt;Any inquiry into the appropriate collection and flow of data should attempt to answer these questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Collection: who can collect and who can decide over future collection?&lt;/li&gt;
  &lt;li&gt;Access: who can access and who can decide over future access?&lt;/li&gt;
  &lt;li&gt;Use: who can use and who can decide over future use?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first question acknowledges that the very act of recording data can have far-reaching consequences. For one, it’s hard to erase data once it is collected, such that collection always implies use (at a minimum, the storage of data). Secondly, the act of recording itself can be viewed as violating our autonomy. Humans behave differently when they know they’re on camera, or when we assume our everyday conversations are on the record (an ever-more reasonable assumption).&lt;/p&gt;

&lt;p&gt;The second and third questions determine how information is used and distributed, once it is collected. In addition to determining who has access and can use data today, we need to know who can make as-yet-unspecified future decisions about future access and use. For example, you may have access to data about you, but do not enjoy the right to decide who else can access that data. Alternatively, it could be entirely up to you to decide who can use data about you, or some specific dataset, and you can revoke that use right whenever you so desire.&lt;/p&gt;

&lt;p&gt;Clearly, the power to decide who can collect, access and use data is more important than merely holding collection, access and use rights right now. Which begs the question: who gets to make those decisions about our data? Oftentimes, the de facto answer to this question is ‘a corporation’, be it Google or Facebook or Amazon. Most of the sensors collecting data are under corporate control and most of the resulting data is held by corporations as well. Especially in jurisdictions without explicit data protection legislations, this reality has meant that corporations decide what data is collected and who can access and use the collected data, and for what purpose. Even when data is collected within the context of a public project (e.g. smart cities) it is often the consulting corporation deciding what to collect, who could access it, how it was used, and by whom — with little public oversight. That’s a problem. The director of a corporation has a fiduciary responsibility to act in the interests of their shareholders. Their job is not to ensure your privacy or to make data available for the public good, but to make money. In fact, even when a company’s shareholders decide they do want to put those values above their need to turn a profit, we cannot trust they will continue to do so in the future. What happens to their good intentions when their corporation is sold?&lt;/p&gt;

&lt;p&gt;Privacy policies coming into force today solve part of the problem, by handing individuals the right to decide how they want to share or not share data about them, and what they allow to be collected in the first place. However, our ability to exercise these rights depends on whether those decisions are made in freedom. Unfortunately, our reliance on a handful of social media platforms and digital services have resulted in power imbalances that undermine any meaningful notion of consent. Our ability to freely choose how and when we share our data breaks down when the ‘choice’ is between surrendering data about ourselves and social exclusion, or even unemployment (as is the case when we decide to opt out of workplace surveillance). Without a real way to opt out, our consent is meaningless.&lt;/p&gt;

&lt;p&gt;Meanwhile, the enforcement of privacy policies leaves much to be desired. Many enforcement bodies rely on complaints, instead of preemptive audits, and are severely understaffed.&lt;/p&gt;

&lt;p&gt;In relation to the questions posed above, data protection laws give us the rights we need to grant and revoke access to and use of data. However, without addressing the underlying power imbalances we remain ill-equipped to exercise those rights.&lt;/p&gt;

&lt;h2 id=&quot;how-to-level-the-playing-field&quot;&gt;How to level the playing field?&lt;/h2&gt;

&lt;p&gt;Three alternative solutions have been proposed to level the playing field. Some look to antitrust laws to break up Big Tech. The idea is that many smaller tech companies would allow for more choice between services. &lt;a href=&quot;https://www.theguardian.com/commentisfree/2019/apr/23/big-tech-google-facebook-unions-public-ownership&quot;&gt;This solution is flawed&lt;/a&gt;. For one, services like search or social media benefit from network effects. Having large datasets to train on, means search recommendations get better. Having all your friends in one place, means you don’t need five apps to contact them all. I would argue those are all things we like and might lose when Big Tech is broken up. What we want is to be able to leave Facebook and still talk to our friends, instead of having many Facebooks. At the same time, more competition is likely to make things worse. When many services need to compete for your attention, it’s in their best interest to make those services as addictive as possible. This cannot be the desired outcome.&lt;/p&gt;

&lt;p&gt;Instead of creating more competition, &lt;a href=&quot;https://www.theguardian.com/commentisfree/2017/aug/30/nationalise-google-facebook-amazon-data-monopoly-platform-public-interest&quot;&gt;some argue we should just nationalize Big Tech&lt;/a&gt;. This strategy leaves us with two important questions: which government should do the nationalizing? And do we want a government in control of data about us?&lt;/p&gt;

&lt;p&gt;Finally, we could decide to divorce those who wish to use data from those who control its use. Personal Data Stores (eg &lt;a href=&quot;https://solid.mit.edu/&quot;&gt;Solid&lt;/a&gt;, or &lt;a href=&quot;https://mydata.org/&quot;&gt;MyData&lt;/a&gt;) aim to do just that. By placing the data with the internet user, rather than the service provider, they hope to put the user back in control. This approach holds a lot of merit. However, it fails to account for our limited ability to decide how we would want to share data. Do we have enough knowledge and insight to weigh our options? And even if we did, do we really want to spend our time making those decisions?&lt;/p&gt;

&lt;h2 id=&quot;data-trusts&quot;&gt;Data Trusts&lt;/h2&gt;

&lt;p&gt;As with personal data stores, by placing data in a data trust we separate the data users from those who control the data. The difference is that with a trust, we avoid placing the entire burden of decision-making on the individual. Moreover, by pooling data from various sources together in a data trust, we unlock the ability for a data trustee to negotiate on behalf of the collective, rather than an individual.&lt;/p&gt;

&lt;p&gt;A data trust is created when someone or a lot of someones hand over their data assets or data rights to a trustee. That trustee can be a person or an organisation, who will then hold and govern that data on behalf of a group of beneficiaries and will do so for a specific purpose. The beneficiaries could be those who handed the data to the trust, or anyone else (including society at large). Importantly, the trustee has a fiduciary responsibility to look out for the interests of the beneficiary, much like your doctor has a fiduciary responsibility to do what is best for you. That also means that the trustee is not allowed to have a profit motive or, more generally, a conflicting interest in the data or data rights under its custody.&lt;/p&gt;

&lt;p&gt;One important feature of a data trust is that the trustee can decide who has access to the data under the trust’s control and who can use it. And, importantly, if that data user fails to comply with the terms and conditions, the trustee can revoke access. To return to the Uber example, instead of you leaving Uber in protest, a trustee can threat to revoke access to the data of many. Such a threat will carry a lot more weight than the act of a single user.&lt;/p&gt;

&lt;h2 id=&quot;the-road-ahead&quot;&gt;The Road Ahead&lt;/h2&gt;

&lt;p&gt;How do we get from here to a world in which our data is governed by data trusts? Needless to say there is still a lot to figure out. How do trustees make decisions about data collection and access? How do we make sure we can continue to trust the trust? Are data trusts possible within our current regulatory environment and to what extent does the answer to that question depends on the jurisdiction you are in?&lt;/p&gt;

&lt;p&gt;We will not find the answers to these and many other remaining questions just by theorizing. Instead, we need to test various models in real-world scenarios. As a Mozilla Fellow I hope to contribute to this effort by considering the usefulness of a data trust for two specific scenarios:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Data Donation Platform: &lt;a href=&quot;https://algorithmwatch.org/en/&quot;&gt;AlgorithmWatch&lt;/a&gt; is looking to build a data donation platform that allows users of browsers to donate data on their usage of specific services (eg Youtube, or Facebook) to a platform. That data is then employed to understand how users are targeted by those platforms, or what ads they are being served. Could this data sit in a trust? Who would the trustee be? Who would we want to access and use this data?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Health data: &lt;a href=&quot;https://coverus.health/&quot;&gt;CoverUS&lt;/a&gt;, a US-based health startup is looking to help its members to collect health data about them and use it to gain better access to health services. We want to find out whether a data trust could hold and govern this data.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is our hope that by studying the concept of a data trust in these specific contexts we will learn more about the incentives and constraints of the various pilot partners for participating in a trust and gain a better understanding of the design requirements for a data trust. We further hope to obtain more insights in the regulatory and policy requirements for data trusts to work.
How you can help&lt;/p&gt;

&lt;p&gt;Image by Joshua Rawson Harris, on Unsplash. Edits by &lt;a href=&quot;https://twitter.com/rich_mason_?lang=en&quot;&gt;Rich Mason&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Nov 2019 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/privacy/consent/2019/11/20/Data-Trusts-Why-What-How.html</link>
        <guid isPermaLink="true">http://localhost:4000/privacy/consent/2019/11/20/Data-Trusts-Why-What-How.html</guid>
        
        
        <category>privacy</category>
        
        <category>consent</category>
        
      </item>
    
      <item>
        <title>Data Portability, Federation And Portable Consent</title>
        <description>&lt;p&gt;Online consent is broken. The enclosure of user data by social media platforms and services have resulted in power imbalances that undermine any meaningful notion of consent. Our ability to freely choose how and when we share our data breaks down when the ‘choice’ is between surrendering data about ourselves or social exclusion. A different reality is possible: one where we can leave a social media platform, and take our social graph and data with us; one in which we can vote with our feet, and thereby change the power dynamic between us and the platforms we rely on for our everyday communication.&lt;/p&gt;

&lt;p&gt;The internet itself was conceived of as an “open” platform - where open, in this case, means that the legal entities, the technical solutions used, and the future classes of content, can all be evolved independent of each other. This combines portability and federation. Portability refers to the idea that any single website or network-connected device can be moved between existing entities. Federation means that new websites and devices can be connected without requiring the participation of existing entities. Taken together, the combination of portability and federation can provide a robust set of protections for individual and societal freedoms.&lt;/p&gt;

&lt;p&gt;Under the General Data Protection Regulation that came into effect in Europe last year, individuals gained a right to data portability. We can now decide to leave Facebook and take our personal data with us to another service. In theory, we are no longer beholden to one social media platform or banking app and are free to break out of the walled gardens that have held us hostage. In reality, we rarely exercise our right to data portability. For starters, it’s hard to actually obtain the data about us. Additionally, data derived from one service is often not directly usable for another.&lt;/p&gt;

&lt;p&gt;What mechanisms and infrastructure need to be in place to realize true portability and federation?&lt;/p&gt;

&lt;h2 id=&quot;the-separation-of-powers-protocol-platform-and-license&quot;&gt;The separation of powers: protocol, platform and license&lt;/h2&gt;

&lt;p&gt;We can divide the problem of providing portability into three segments: Protocol, Platform, and License. We argue that data portability and federation requires each of these segments to be created independent of the others, such that no single segment can ever prescribe rules to the remaining two.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Protocol&lt;/strong&gt; describes the relationship between data and the set of explicit consents that have been granted on that data. Take the git protocol as an example, which describes the history of changes to a data set and the authors of those changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Platform&lt;/strong&gt; is envisioned as a server environment that supports real-time API-driven access to data sets, mediated by a rules engine that conforms to the Protocol. This includes a set of read and write clients, as well as standalone policy engines that enforce pre-collection permissions and post-read usage and summarization consent. All of the components of the platform are designed to allow robust audit capabilities, and a tamper-proof activity log.&lt;/p&gt;

&lt;p&gt;To return to the git example, this open protocol works with various platforms, like GitHub and GitLab. If users of one of these platforms stop trusting in its governance, they can easily take their code base to a different platform. In the extreme case, they can run a platform themselves.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;License&lt;/strong&gt; provides a permanently-attached set of limits on usage and derived work, and functions exactly the same as copy-left or permissive licensing in the open source world. Note that extended permutations of the copy-left principles (including licenses that explicitly prohibit commercial activity) are likely to emerge in this space. Additionally, licenses and other legal frameworks are the only portion of a data trust structure that remains ATTACHED to the data after it is exported; thus, they are critical to protecting the integrity of the consent(s) during migrations.&lt;/p&gt;

&lt;h2 id=&quot;portable-consent-an-unavoidable-complexity&quot;&gt;Portable Consent: An unavoidable complexity&lt;/h2&gt;

&lt;p&gt;When transferring data between services, or organisations, we need to be able to ensure that the privacy statements attached to the data are upheld. In other words, if data is moved from one social media to another, a user should be able to trust that the privacy settings agreed to on the first platform, equally apply on the second. This means that the mechanisms for managing consent need to exist at the protocol and license layer, decoupled from the platform.&lt;/p&gt;

&lt;p&gt;In order for protocols to provide meaningful portability guarantees, there must be at least one platform available that implements this protocol (such as GIT for the git protocol, httpd for the HTTP protocol) and that platform must be a) operated by at least two different entities, and b) be relatively simple for an additional entity to operate. Any data trust operator could swap out the underlying platform without breaking portability, provided the same underlying protocol was supported.&lt;/p&gt;

&lt;p&gt;Moreover, the entity that defines and evolves the protocol should ideally be separate from any of the entities that operate these platforms - much in the same way the W3C is separate from any specific browser.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Each of the three aspects of portability represents a check or balance on the others. Licenses and protocols can both be abused to tightly couple data to a specific platform; similarly, protocols can be structured in a way to limit the potential licenses with which they could be used. Users and their consent proxies should remain free to select the license of their choice, unconstrained by platform or protocol limitations.&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jul 2019 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/privacy/consent/2019/07/03/data-portability.html</link>
        <guid isPermaLink="true">http://localhost:4000/privacy/consent/2019/07/03/data-portability.html</guid>
        
        
        <category>privacy</category>
        
        <category>consent</category>
        
      </item>
    
      <item>
        <title>Could consent champions help us navigate privacy concerns?</title>
        <description>&lt;p&gt;“If no one reads the terms and conditions, how can they continue to be the backbone of the  internet?” asks the New York Times editorial board in an article titled &lt;a href=&quot;https://www.nytimes.com/2019/02/02/opinion/internet-facebook-google-consent.html&quot;&gt;‘How Silicon Valley puts the ‘con’ in consent’&lt;/a&gt;. Despite data protection laws becoming commonplace, we have yet to find consent models beyond those that rely on individual users blindly clicking ‘Agree’ or opting into cookies they hardly understand. Online consent is severely broken and the wreckage extends beyond the impossible-to-navigate consent windows. &lt;a href=&quot;https://www.amacad.org/publication/contextual-approach-privacy-online&quot;&gt;Helen Nissenbaum&lt;/a&gt; argues that the model of notice-and-consent is inherently flawed as we can never fully understand the repercussions of data about us being used in different contexts:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Proposals to improve and fortify notice-and-consent, such as clearer privacy policies and fairer information practices, will not overcome a fundamental flaw in the model, namely, its assumption that individuals can understand all facts relevant to true choice at the moment of pair-wise contracting between individuals and data gatherers.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;One of the underlying problems, Nissenbaum observes, is that consent is not meaningful when we cannot trust the system in place to protect our rights. We can, for example, meaningfully consent to have a surgeon operate on our kidneys; not because we have a perfect understanding of what such an operation might entail, but because we trust the medical system works and protects us. The only decisions left for us are not the specific way we want to surgeon to cut our kidney, but whether we have any problems with blood transfusions or resuscitation. Decisions that depend on our norms and beliefs; things we understand.&lt;/p&gt;

&lt;p&gt;“Choosing is not mere picking but requires that the subject understands that to which he or she is consenting”. Especially when considering raw data, like the number of keystrokes, or seconds spend staring at a screen, context is missing. The data lacks sufficient meaning to allow us to understand possible privacy implications of sharing such data. Only once we think through specific use cases and combinations of data do we start to understand how sharing it could help or hinder us. Such an analysis, however, requires extensive time and domain knowledge.&lt;/p&gt;

&lt;p&gt;But we should not have to be experts to make decisions about how data about us is collected and shared. Likewise, we should not have to trust Facebook to make those decisions for us. One solution is to play safe, and severely limit the data that is collected about us. This is the approach taken by the General Data Protection Regulation (GDPR) that came into force in Europe last year. Whilst an improvement, this defensive attitude towards data sharing does not help us in circumstances where we might want to share more data about ourselves: where doing so is in both our personal and collective interests.&lt;/p&gt;

&lt;h2 id=&quot;consent-champions&quot;&gt;Consent champions&lt;/h2&gt;

&lt;p&gt;Enter consent champions: organisations to make consent decisions on your behalf. These could be any organisation you trust to make decisions for you on specific types of data about you. For instance, someone infected with HIV might have specific privacy preferences, ranging from a desire to keep this information private, to wishes to help research towards better treatments. HIV advocacy bodies would potentially be well-suited to understand the context necessary to navigate these concerns, which are both complex and interlinked. In a similar vein, trade unions could be well-positioned to create consent profiles that would specifically deal with data collection and sharing relating to future employment, or wage calculations.&lt;/p&gt;

&lt;p&gt;Consent champions, therefore, are organisations that hold expertise in specific domains (e.g. health, mobility, human rights) and use that expertise to draft consent profiles regarding data collection and usage that individuals can adopt as their own. These profiles reflect the ethical considerations and risk assessments performed by the consent champions, as well as the values and norms the proxy stands for. The trust placed in consent champions is founded in their proven capacity for understanding the specific norms, values, needs and expectations of a specific demographic, given a specific context.&lt;/p&gt;

&lt;h2 id=&quot;requirements-for-effective-consent-champions&quot;&gt;Requirements for effective consent champions&lt;/h2&gt;
&lt;p&gt;In order for a consent proxy to effectively address the problems laid out above, it needs to fulfill a number of roles and requirements:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Consent profiles should be usable&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The user should be presented, by the proxy they are entrusting with their data decisions, with a menu of possible consent profiles. The consent profiles should provide an easy overview of the main norms, functions and goals that underlies them, as well as a clear overview of the types of data the consent profile cover. It thereby replaces the spider web of choices users are currently faced with (for instance in the form of Facebook’s privacy settings). Underneath this easy-to-navigate profile sits a fine-grained set of rules that govern the relationship between the platform users and those collecting, using and sharing data about them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Variety of consent&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;To reflect the fact that human beings hold many identities that are subject to change, we should be able to a) choose from a wide range of consent champions and b) elect different consent champions to govern different types of data, as well as different types of concerns. For instance, we might want to have health data about us governed by both our GP and the Diabetes Foundation. As such, different profiles could end up governing the same piece of data. In those cases it makes sense for the most stringent privacy preferences to become the default, unless otherwise stipulated. In order to automatically negotiate the differences between various profiles, their rules should be available in a computer readable format. The result of these various compatible data proxies and profiles, working in concert is a set of API rules dictating what data can and cannot be collected, stored and used by data users, and under what conditions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Low barriers of entry and exit&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;As our values and circumstances are subject to change, we should be able to switch between consent champions easily. This, in addition, allows us to vote with our feet.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;would-consent-champions-work-in-the-wild&quot;&gt;Would consent champions work in the wild?&lt;/h2&gt;

&lt;p&gt;At this point, the careful reader will wonder why any platform or service would agree to have their activities curtailed by consent profiles, when at the moment they have free rein. Fair question. For the moment, consent profiles are likely most relevant to services that rely on their users trusting them with data about them. It could, for instance, be a solution for companies that allow users to make data about them available to third parties. What if citizens would like to make location data about them available to local governments to improve flow of traffic? Or perhaps salaried workers would like their salary data available for research into gender disparities? A consent proxy could be a great way to help them better share data about themselves, while ensuring the way this data is used corresponds with their values and privacy expectations.&lt;/p&gt;

&lt;p&gt;But to make consent champions work for the average internet user, the creation of profiles alone will not be enough. Consent champions would be toothless without ways to enforce the profiles they create. Their power relative to, for instance, a social media platform depends on their ability to steer individuals away from platforms that do not comply with their consent profiles. At the moment that is unlikely to be the case. We may not want to hand over data about us to Facebook, but we are equally unwilling to give up our social network. This problem will not be solved by a consent proxy, but will instead require true data portability - the ability to take data about us from one platform to the next.&lt;/p&gt;

&lt;p&gt;A number of solutions to level the playing field have already been proposed, ranging from decentralized web technologies (e.g. Solid, Holochain, MaidSafe) to data trusts. While still in their infancy, these technologies and infrastructures promise to shift the power from the creators of closed platforms, back to the consumers of those platforms. Once the playing field is sufficiently leveled, a consent proxy - or consortium of consent champions - would hold enough collective bargaining power to alter the behavior of data collectors and users. Equally, consent champions could meaningfully advise against using a platform that fails to adopt any of its profiles.&lt;/p&gt;

&lt;h2 id=&quot;to-conclude&quot;&gt;To conclude&lt;/h2&gt;

&lt;p&gt;Consent champions will not be a magic bullet, but rather part of a range of infrastructural solutions that together pave the way for better data sharing while safeguarding individual and collective privacy. Of course, there are a number of remaining questions to work out. Who would cover the costs of setting up the consent champions and creating the consent profiles? How do we ensure that the organisations we trust to become these proxies have enough understanding of data (in addition to their understanding of their specific domain)?&lt;/p&gt;

&lt;p&gt;One of the great failures in the current data and privacy debate is the users who are routinely set up to fail; expected to fend for themselves and make wise data choices with too little information, even if they have the understanding. Much like we do not need to ask whether the water we drink is clean, everytime we take a sip, we should not have to evaluate a wide range of privacy concerns everytime we log into an app. Trusted consent champions, with area-relevant expertise, would be one way to relieve this burden.&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Jun 2019 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/privacy/consent/2019/06/24/consent-champions.html</link>
        <guid isPermaLink="true">http://localhost:4000/privacy/consent/2019/06/24/consent-champions.html</guid>
        
        
        <category>privacy</category>
        
        <category>consent</category>
        
      </item>
    
      <item>
        <title>Why Your Privacy Is About All Of Us</title>
        <description>&lt;p&gt;We are on the record. With or without our consent, data about us has been recorded, stored, analysed and used to predict our behaviours and gain insights into our brightest dreams and darkest secrets. Data is nothing more than our recorded actions and words: it is not good or bad in its own right. Data can give us the information we need to cure cancer, or it can be weaponised and used to steer our behaviours. What is problematic is not that data can be exploited, but that we never consented to the exploitation. When we signed up for Facebook, we were never asked whether it would be alright if the platform shared what it had learned about us with political campaigners and advertisers. Actors who, unbeknownst to most of us, went on to target disinformation campaigns at those most susceptible to their messaging.&lt;/p&gt;

&lt;p&gt;The answer seems obvious: we need to give the user more control over the data they share and more insight into how it might be used. It is the solution put forth by many a privacy advocate. Unfortunately, while it seems sensible, this approach is misguided — or at the very least incomplete. Here’s why. Remember that time a marketing stunt unintentionally revealed the location of secret US military bases? It happened in late 2017, when Strava, an app that enables runners and cyclists to track their exercise routes, aggregated all the routes generated by its users in a data visualisation map for all the world to see. What it did not realise was that some of its users were US soldiers, who had used the app while running around their military bases. Things started to unravel when an Australian university student spotted the routes in remote areas, such as Afghanistan, and quickly determined that these patterns revealed US military bases. He was right.&lt;/p&gt;

&lt;p&gt;This example shows how difficult it is to foresee the consequences of us, consensually, sharing data. The problem is that most of the data we share is not just about us, it’s often about other people and things as well. This is as true for our running routes as it is for our text messages, family pictures and DNA. Say, in a moment of self-discovery, you decided to send your DNA to an analyst. You would not just be sharing data about you. You would also be sharing data about your family and their future offspring. Should it be up to you alone to decide who collects this data?&lt;/p&gt;

&lt;p&gt;Another example. Some UK-based health insurers have started to give rewards to people who volunteer information about themselves. Did you connect your Fitbit to the insurance app? Here’s a free cinema ticket. Can you prove you went to a gym today? A free cappuccino. At first sight, this scheme appears harmless. Nobody is forced to share data and those who do get fun perks without penalties. Now fast-forward five years. By now, all healthy, tech-savvy individuals have opted into the service, enjoying their many caffeinated movies. But what does this tell us about those who have not opted in? Can we assume their unwillingness to upload their fitness logs means they are less healthy? What is to stop an insurance company from charging them higher rates? Clearly, even when sharing data makes sense to somebody on an individual level, we should be mindful of the negative externalities to society as a whole.&lt;/p&gt;

&lt;p&gt;If individual consent is insufficient, what is the alternative? Just as it would be ludicrous to expect any one person to individually evaluate whether the air they breathe is toxic every time they inhale, we cannot expect people to make a million tiny decisions about their data on a daily basis. Strong defaults and safety standards are imperative. Rather than place the burden of consent solely on the shoulders of the individual, we should collectively decide what data we want to collect and give access to, and under what conditions. Such an exercise should weigh the interests of different groups, while protecting the most vulnerable. Left-wing movements have a history of navigating the tensions between collective harm and individual freedom. It’s time for their voice to be heard in this debate as well. Ultimately, can we really expect individuals to make more informed decisions than an organisation with the analytical resources of the US army?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://labourlist.org/2019/02/data-protection-and-individual-consent-why-your-privacy-is-about-all-of-us/&quot;&gt;Originally published on Labourlist.&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Feb 2019 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/privacy/consent/2019/02/18/why-your-privacy-is-about-all-of-us.html</link>
        <guid isPermaLink="true">http://localhost:4000/privacy/consent/2019/02/18/why-your-privacy-is-about-all-of-us.html</guid>
        
        
        <category>privacy</category>
        
        <category>consent</category>
        
      </item>
    
  </channel>
</rss>
